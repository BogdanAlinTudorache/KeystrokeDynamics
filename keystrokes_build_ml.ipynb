{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd",
      "metadata": {
        "trusted": true
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def calculate_mean_and_standard_deviation(feature_list):\n    from math import sqrt\n    # calculate the mean\n    mean = sum(feature_list) / len(feature_list)\n\n    # calculate the squared differences from the mean\n    squared_diffs = [(x - mean) ** 2 for x in feature_list]\n\n    # calculate the sum of the squared differences\n    sum_squared_diffs = sum(squared_diffs)\n\n    # calculate the variance\n    variance = sum_squared_diffs / (len(feature_list) - 1)\n\n    # calculate the standard deviation\n    std_dev = sqrt(variance)\n\n    return mean, std_dev\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def process_csv(df_input_csv_data):\n    data = {\n        'user': [],\n        'ht_mean': [],\n        'ht_std_dev': [],\n        'ppt_mean': [],\n        'ppt_std_dev': [],\n        'rrt_mean': [],\n        'rrt_std_dev': [],\n        'rpt_mean': [],\n        'rpt_std_dev': [],\n    }\n\n    # iterate over each row in the dataframe\n    for i, row in df_input_csv_data.iterrows():\n        # iterate over each pair of consecutive presses and releases\n        # print('user:', row['user'])\n\n        # list of hold times\n        ht_list = []\n        # list of press-press times\n        ppt_list = []\n        # list of release-release times\n        rrt_list = []\n        # list of release-press times\n        rpt_list = []\n\n        # I use the IF to select only the X rows  of the csv\n        if i < 885:\n            for j in range(12):\n                # calculate the hold time: release[j]-press[j]\n                ht = row[f\"release-{j}\"] - row[f\"press-{j}\"]\n                # append hold time to list of hold times\n                ht_list.append(ht)\n\n                # calculate the press-press time: press[j+1]-press[j]\n                if j < 11:\n                    ppt = row[f\"press-{j + 1}\"] - row[f\"press-{j}\"]\n                    ppt_list.append(ppt)\n\n                # calculate the release-release time: release[j+1]-release[j]\n                if j < 11:\n                    rrt = row[f\"release-{j + 1}\"] - row[f\"release-{j}\"]\n                    rrt_list.append(rrt)\n\n                # calculate the release-press time: press[j+1] - release[j]\n                if j < 10:\n                    rpt = row[f\"press-{j + 1}\"] - row[f\"release-{j}\"]\n                    rpt_list.append(rpt)\n\n            # ht_list, ppt_list, rrt_list, rpt_list are a list of calculated values for each feature -> feature_list\n            ht_mean, ht_std_dev = calculate_mean_and_standard_deviation(ht_list)\n            ppt_mean, ppt_std_dev = calculate_mean_and_standard_deviation(ppt_list)\n            rrt_mean, rrt_std_dev = calculate_mean_and_standard_deviation(rrt_list)\n            rpt_mean, rpt_std_dev = calculate_mean_and_standard_deviation(rpt_list)\n            # print(ht_mean, ht_std_dev)\n            # print(ppt_mean, ppt_std_dev)\n            # print(rrt_mean, rrt_std_dev)\n            # print(rpt_mean, rpt_std_dev)\n\n            data['user'].append(row['user'])\n            data['ht_mean'].append(ht_mean)\n            data['ht_std_dev'].append(ht_std_dev)\n            data['ppt_mean'].append(ppt_mean)\n            data['ppt_std_dev'].append(ppt_std_dev)\n            data['rrt_mean'].append(rrt_mean)\n            data['rrt_std_dev'].append(rrt_std_dev)\n            data['rpt_mean'].append(rpt_mean)\n            data['rpt_std_dev'].append(rpt_std_dev)\n\n        else:\n            break\n    data_df = pd.DataFrame(data)\n    return data_df\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def train_svm(training_data, features):\n    import joblib\n    from sklearn.svm import SVC\n\n    \"\"\"\n    SVM stands for Support Vector Machine, which is a type of machine learning algorithm used:\n    for classification and regression analysis.\n\n    SVM algorithm aims to find a hyperplane in an n-dimensional space that separates the data into two classes.\n    The hyperplane is chosen in such a way that it maximizes the margin between the two classes, \n    making the classification more robust and accurate.\n\n    In addition, SVM can also handle non-linearly separable data by mapping the original features to a\n     higher-dimensional space, where a linear hyperplane can be used for classification.\n    \n    :param training_data:\n    :param features:\n    :return: ML Trained model\n    \"\"\"\n\n    # Split the data into features and labels\n    X = training_data[features]\n    y = training_data['user']\n\n    # Train an SVM model on the data\n    svm_model = SVC()\n    svm_model.fit(X, y)\n\n    # Save the trained model to disk\n    svm_model_name = 'models/svm_model.joblib'\n    joblib.dump(svm_model, svm_model_name)\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def train_random_forest(training_data, features):\n    \"\"\"\n    Random Forest is a type of machine learning algorithm that belongs to the family of ensemble learning methods.\n    It is used for classification, regression, and other tasks that involve predicting an output value based on\n    a set of input features.\n\n    The algorithm works by creating multiple decision trees, where each tree is built using a random subset of the\n    input features and a random subset of the training data. Each tree is trained independently,\n    and the final output is obtained by combining the outputs of all the trees in some way, such as taking the average\n    (for regression) or majority vote (for classification).\n\n\n    :param training_data:\n    :param features:\n    :return: ML Trained model\n    \"\"\"\n    import joblib\n    from sklearn.ensemble import RandomForestClassifier\n\n    # Split the data into features and labels\n    X = training_data[features]\n    y = training_data['user']\n\n    # Train a Random Forest model on the data\n    rf_model = RandomForestClassifier()\n    rf_model.fit(X, y)\n\n    # Save the trained model to disk\n    rf_model_name = 'models/rf_model.joblib'\n    joblib.dump(rf_model, rf_model_name)\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def train_xgboost(training_data, features):\n    import joblib\n    import xgboost as xgb\n    from sklearn.preprocessing import LabelEncoder\n    \"\"\"\n    XGBoost stands for Extreme Gradient Boosting, which is a type of gradient boosting algorithm used for classification and regression analysis.\n    XGBoost is an ensemble learning method that combines multiple decision trees to create a more powerful model.\n    Each tree is built using a gradient boosting algorithm, which iteratively improves the model by minimizing a loss function.\n    XGBoost has several advantages over other boosting algorithms, including its speed, scalability, and ability to handle missing values.\n\n    :param training_data:\n    :param features:\n    :return: ML Trained model\n    \"\"\"\n\n    # Split the data into features and labels\n    X = training_data[features]\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(training_data['user'])\n\n    # Train an XGBoost model on the data\n    xgb_model = xgb.XGBClassifier()\n    xgb_model.fit(X, y)\n\n    # Save the trained model to disk\n    xgb_model_name = 'models/xgb_model.joblib'\n    joblib.dump(xgb_model, xgb_model_name)\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def main():\n    # Load CSV\n    df_csv = pd.read_csv('Train_keystroke.csv')\n    training_data = process_csv(df_csv)\n    # List of features we want train on\n    features = ['ht_mean', 'ht_std_dev', 'ppt_mean', 'ppt_std_dev', 'rrt_mean', 'rrt_std_dev', 'rpt_mean',\n                'rpt_std_dev']\n\n    # This function  trains an SVM machine learning model on the training data using the specified features.\n    train_svm(training_data, features)\n    # This function  trains a Random Forest machine learning model on the training data using the specified features.\n    train_random_forest(training_data, features)\n    # This function  trains an XGBoost machine learning model on the training data using the specified features.\n    train_xgboost(training_data, features)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "main()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}